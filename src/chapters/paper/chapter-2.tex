\section{Related Work and Theoretical Foundation}

\subsection{Related Research}
Previous research in financial document extraction has explored various approaches with mixed results. Earlier studies using OCR-based solutions demonstrated the challenges inherent in traditional document processing pipelines. Smith and Johnson \cite{penelitian1} developed an OCR and machine learning combination for invoice data extraction, achieving reasonable accuracy but requiring complex preprocessing steps that often failed with varying document qualities and formats. Their approach highlighted the brittleness of OCR-dependent systems when dealing with real-world document variations such as lighting conditions, image quality, and font variations.

Chen and Kumar \cite{penelitian2} explored CNN-based classification of financial documents with machine learning enhancement, achieving 87\% accuracy in their controlled dataset. However, their system was limited to homogeneous document formats and struggled with the format diversity commonly found in real-world payment receipts. The research revealed significant performance degradation when applied to documents outside their training distribution, particularly when dealing with mobile-captured images with varying backgrounds, orientations, and lighting conditions.

Recent advances in transformer-based document understanding have shown promising results for complex document analysis tasks. LayoutLM \cite{layoutlm} combines visual and textual information for document understanding, demonstrating superior performance in document classification and information extraction tasks. BERT \cite{bert} provides a strong foundation for natural language processing components in document analysis systems. However, these approaches still require OCR preprocessing stages that introduce error propagation and increase system complexity, particularly when dealing with mobile-captured images of varying quality.

\subsection{Document Understanding Transformer (Donut)}
Donut (Document Understanding Transformer) represents a paradigm shift in document analysis by providing state-of-the-art end-to-end information extraction without requiring OCR preprocessing \cite{donut}. The model architecture consists of two main components working in tandem to process document images directly into structured output.

The visual encoder utilizes Swin Transformer architecture to extract visual features from document images. This component processes the entire document image as a sequence of patches, learning to identify and encode visual patterns, layouts, and textual elements without explicit text recognition. The Swin Transformer's hierarchical approach allows the model to capture both local details and global document structure, making it particularly effective for documents with complex layouts like payment receipts.

The textual decoder employs BART (Bidirectional and Auto-Regressive Transformers) to generate structured output in JSON format. This component translates the visual features extracted by the encoder into meaningful structured data, learning to map visual patterns to semantic content. The decoder's autoregressive nature allows it to generate variable-length outputs and handle different document types with varying information structures.

The key advantage of the Donut model compared to traditional approaches lies in its ability to understand document structure holistically without the OCR error propagation that affects pipeline-based systems. This end-to-end learning approach enables the model to develop robust representations that work effectively even with degraded image quality, varied lighting conditions, and complex layouts commonly encountered in mobile-captured payment receipts.

\subsection{Development Framework and Architecture}
The system implementation leverages modern development frameworks optimized for mobile-first applications. Flutter provides the foundation for cross-platform mobile development, enabling native performance while maintaining a single codebase. Flutter's reactive programming model and extensive widget library support rapid development of intuitive user interfaces that meet Gen Z expectations for mobile application design and responsiveness.

FastAPI serves as the backend framework, providing efficient REST API interfaces with automatic documentation generation and built-in data validation. The framework's asynchronous request handling capabilities are particularly important for machine learning inference workloads, allowing the system to handle multiple concurrent requests while maintaining responsive performance. FastAPI's integration with modern Python ML frameworks facilitates seamless deployment of deep learning models in production environments.

Docker containerization ensures deployment consistency across different environments, from development to production. The containerized architecture isolates model dependencies and provides scalable deployment options for cloud platforms. This approach also facilitates model versioning and enables smooth updates without service disruption.

\subsection{Evaluation Methodology and Metrics}
Model performance evaluation employs standard information extraction metrics including Accuracy, Precision, Recall, F1-score, and Match Character Error Rate (MCER). These metrics provide comprehensive assessment of the model's ability to correctly identify and extract relevant information from payment documents. MCER specifically measures the character-level accuracy of extracted text, providing insight into the practical usability of extracted data for expense tracking purposes.

User experience evaluation utilizes the System Usability Scale (SUS), an industry-standard questionnaire for measuring usability on a 0-100 scale. SUS scores above 68 are considered acceptable, with scores above 80 indicating excellent usability. The scale's validated psychometric properties make it suitable for comparing usability across different user groups and application domains, providing reliable insights into user satisfaction and system adoption potential.