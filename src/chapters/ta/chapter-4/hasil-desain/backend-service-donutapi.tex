\subsection{Backend Service DonutAPI}
\label{subsec:backend-service-donutapi}

DonutAPI merupakan implementasi REST API menggunakan framework FastAPI yang menyediakan layanan inferensi model DONUT untuk ekstraksi data dari dokumen pembayaran. Service ini dirancang dengan arsitektur modular yang memisahkan layer API, business logic, dan model inference untuk memudahkan maintenance dan pengembangan future features.

Arsitektur backend mengadopsi design patterns yang robust termasuk singleton pattern untuk model management, dependency injection untuk service components, dan comprehensive error handling untuk production reliability. Service mengimplementasikan automatic model loading saat startup dengan preloading mechanism yang memastikan kedua model siap digunakan sebelum menerima request dari client.

Application lifecycle management menggunakan FastAPI's lifespan context manager yang mengatur startup dan shutdown processes secara graceful. Startup sequence meliputi model preloading, device detection, dan system health verification. Shutdown sequence memastikan proper cleanup dari model resources dan GPU memory clearing jika menggunakan CUDA acceleration.

**Diagram yang disarankan**: Service architecture diagram menunjukkan layer separation, request flow, dan component interactions dalam DonutAPI.

\subsubsection{Endpoint \texttt{/predict/}}
\label{subsubsec:endpoint-predict-full}

Endpoint \texttt{/predict/} menyediakan full prediction service dengan detailed response yang mencakup metadata lengkap tentang inference process. Endpoint ini dirancang untuk use cases yang memerlukan informasi comprehensive tentang model performance dan processing details.

Request validation mengimplementasikan comprehensive checks termasuk file format validation (JPG, JPEG, PNG, WebP, BMP), file size limits maksimum 10MB, dan basic file integrity verification. Query parameter \texttt{model} bersifat required dan harus berupa \texttt{ModelType.BASE} atau \texttt{ModelType.CUSTOM} untuk menentukan model mana yang akan digunakan untuk inference.

Response structure untuk endpoint ini mencakup:
\begin{itemize}
    \item \texttt{success}: Boolean indicator untuk status inference
    \item \texttt{result}: Object containing extracted data dan metadata
    \item \texttt{model\_info}: Informasi detail tentang model yang digunakan
    \item \texttt{processing\_time\_seconds}: Waktu yang diperlukan untuk inference
    \item \texttt{device}: Hardware device yang digunakan untuk inference (CPU/GPU)
    \item \texttt{raw\_sequence}: Token sequence mentah sebelum postprocessing (khusus custom model)
    \item \texttt{confidence\_score}: Skor confidence untuk kualitas ekstraksi (khusus custom model)
\end{itemize}

Error handling untuk endpoint ini mengimplementasikan structured error responses dengan HTTP status codes yang appropriate dan detailed error messages untuk debugging purposes. Request tracing menggunakan unique request ID yang memudahkan correlation antara logs dan monitoring metrics.

\subsubsection{Endpoint \texttt{/predict/simple}}
\label{subsubsec:endpoint-predict-simple}

Endpoint \texttt{/predict/simple} menyediakan simplified response yang focus pada extracted data untuk mobile application consumption. Response format yang streamlined mengurangi bandwidth usage dan mempercepat parsing di sisi client application.

Response structure untuk simple endpoint hanya mencakup essential information:
\begin{itemize}
    \item \texttt{success}: Boolean indicator untuk status inference
    \item \texttt{data}: Object containing extracted data dalam raw format dan cleaned format
    \item \texttt{error}: Error message jika inference gagal
\end{itemize}

Data cleaning dan normalization dilakukan secara otomatis pada endpoint ini untuk memastikan client application menerima data yang ready-to-use. Proses cleaning mencakup currency formatting untuk amount fields, date parsing untuk timestamp fields, dan text normalization untuk name fields.

Custom model response pada endpoint simple mencakup automatic classification result yang menentukan apakah dokumen adalah QRIS atau transfer berdasarkan extracted type field. Base model response menyediakan structured receipt data dengan itemized information yang sudah dinormalisasi untuk mobile display.

\subsubsection{Model Selection dan Inference Service}
\label{subsubsec:model-selection-inference}

DonutAPI mengimplementasikan model selection logic berdasarkan query parameter 'model' yang menentukan apakah menggunakan Internal Model (QRIS-TF) atau Deployed Model (CORD-v2) berdasarkan document type yang akan diproses. Model selection ditentukan oleh receipt type yang dispecify dalam endpoint query parameters, memberikan control yang explicit kepada client application untuk memilih model yang sesuai dengan jenis dokumen.

Deployment architecture menggunakan three-tier approach dengan Android client, Docker container hosting FastAPI service dengan Internal Model, dan external HuggingFace service untuk Deployed Model. Arsitektur ini memungkinkan scalability dan maintainability yang optimal dengan separation between local inference dan cloud-based model serving.

Inference service menggunakan singleton pattern untuk kedua model instances yang memastikan memory efficiency dan menghindari multiple model loading. Model initialization menggunakan lazy loading approach yang hanya memuat model ketika pertama kali dibutuhkan, optimizing startup time dan memory usage.

Error recovery mechanism mengimplementasikan graceful degradation di mana jika primary model gagal, system secara otomatis fallback ke alternative processing atau memberikan informative error message kepada client. Fallback logic untuk custom model dapat menggunakan base model jika custom weights gagal dimuat.

Service juga mengimplementasikan resource monitoring yang track GPU memory usage, CPU utilization, dan inference latency untuk operational insights. Health check endpoints \texttt{/status} dan \texttt{/} menyediakan comprehensive system status termasuk model readiness, hardware information, dan performance metrics.

**Diagram yang disarankan**: API request/response flow diagram menunjukkan validation, model selection, inference, dan response formatting untuk kedua endpoints.

Deployment configuration menggunakan Docker containerization dengan multi-stage builds yang mengoptimalkan image size dan security. Container includes automatic device detection untuk CPU dan GPU environments dengan graceful fallback jika CUDA tidak tersedia. Production deployment menggunakan Uvicorn ASGI server dengan process management yang robust untuk high availability scenarios.
