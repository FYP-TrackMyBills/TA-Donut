\subsubsection{Implementasi Proses \emph{Fine-tuning} Hybrid}
\label{subsubsec:implementasi-proses-fine-tuning-hybrid}

Berdasarkan analisis mendalam terhadap berbagai pendekatan implementasi \emph{fine-tuning}, penulis mengembangkan solusi hybrid yang mengombinasikan praktik terbaik dari beberapa strategi implementasi yang telah diuji. Pendekatan hybrid ini bertujuan untuk mengoptimalkan proses \emph{fine-tuning} dengan mempertimbangkan aspek robustness, efisiensi memori, dan kualitas hasil pelatihan. Implementasi ini menggabungkan penanganan model yang robust, strategi loading data yang efisien, sistem monitoring yang komprehensif, dan optimisasi memori yang efektif untuk mencapai hasil pelatihan yang optimal dalam lingkungan dengan keterbatasan sumber daya komputasi.

Arsitektur hybrid yang dikembangkan mengintegrasikan empat komponen utama yang saling melengkapi untuk memastikan stabilitas dan efektivitas proses pelatihan. Komponen pertama adalah penanganan model dan processor yang robust, yang mengadopsi strategi error handling komprehensif untuk setup model awal, termasuk validasi vocabulary expansion dan konfigurasi token khusus. Komponen kedua adalah strategi loading data yang efisien menggunakan pendekatan direct JSONL loading yang menghindari preprocessing berlebihan sambil mempertahankan fleksibilitas dalam penanganan format data yang tidak konsisten. Komponen ketiga mempertahankan collation logic yang telah terbukti efektif dalam handling decoder input dan label masking. Komponen keempat mengimplementasikan sistem monitoring komprehensif dengan session keep-alive mechanism untuk mencegah timeout pada training jangka panjang, terutama penting untuk environment cloud computing yang memiliki batasan waktu session.

Proses setup model dirancang menggunakan pendekatan yang \textit{robust} dengan penanganan \textit{error} berlapis untuk memastikan stabilitas proses inisialisasi dan konfigurasi. Model dasar yang digunakan adalah \donutcord, yang kemudian \textit{vocabulary}-nya diperluas untuk menambahkan 14 token khusus domain. Token-token ini terdiri dari 12 token untuk pasangan \textit{field} yang mencakup tag pembuka dan penutup untuk setiap target ekstraksi seperti

 \texttt{total\_amount}, 
 
%  \texttt{transaction\_time}, \texttt{transaction_identifier}, \texttt{type}, \texttt{target_name}, dan \texttt{application}, serta 2 token untuk identifikasi tugas, yaitu \texttt{<s_payment_proof>} dan \texttt{</s_payment_proof>}. 
 
Proses ekspansi \textit{vocabulary} ini memerlukan penyesuaian teknis berupa \textit{resize token embeddings} pada \textit{decoder} dan pembaruan konfigurasi model untuk memastikan sinkronisasi ukuran \textit{vocabulary} di semua komponennya. Sebagai pelengkap, konfigurasi proses generasi juga diatur dengan menetapkan 

% \texttt{decoder_start_token_id} agar konsisten merujuk pada token \texttt{<s_payment_proof>}, sehingga memberikan konteks yang jelas kepada model bahwa tugas yang dijalankan adalah ekstraksi informasi dari dokumen bukti pembayaran.

% , \texttt{transaction_time}, \texttt{transaction_identifier}, \texttt{type}, \texttt{target_name}, dan \texttt{application}, serta 2 token untuk identifikasi tugas, yaitu \texttt{<s_payment_proof>} dan \texttt{</s_payment_proof>}. Proses ekspansi \textit{vocabulary} ini memerlukan penyesuaian teknis berupa \textit{resize token embeddings} pada \textit{decoder} dan pembaruan konfigurasi model untuk memastikan sinkronisasi ukuran \textit{vocabulary} di semua komponennya. Sebagai pelengkap, konfigurasi proses generasi juga diatur dengan menetapkan \texttt{decoder_start_token_id} agar konsisten merujuk pada token \texttt{<s_payment_proof>}, sehingga memberikan konteks yang jelas kepada model bahwa tugas yang dijalankan adalah ekstraksi informasi dari dokumen bukti pembayaran.

Implementasi dataset hybrid mengombinasikan direct JSONL loading dengan logic preprocessing yang robust untuk menangani berbagai kondisi data yang tidak ideal yang sering ditemukan dalam dataset real-world. Strategi loading ini menghindari preprocessing berlebihan yang dapat memperlambat proses pelatihan sambil tetap mempertahankan kemampuan untuk menangani format data yang tidak konsisten, termasuk file metadata yang mengalami korupsi atau format yang tidak sesuai standar. Sistem pencarian file gambar yang fleksibel diimplementasikan dengan multiple fallback mechanism untuk mengatasi inkonsistensi penamaan file, termasuk pencocokan direct path, case-insensitive matching, dan pencocokan dengan berbagai ekstensi gambar seperti jpg, jpeg, dan png. Dataset ini juga mendukung augmentation opsional berupa brightness adjustment untuk meningkatkan generalisasi model, serta fault tolerance yang robust untuk handling missing files dan corrupted annotations dengan fallback ke dummy data yang memungkinkan training tetap berlanjut meskipun ada sample yang bermasalah.

Strategi optimisasi memori dan pelatihan menggunakan beberapa teknik canggih untuk mengatasi keterbatasan sumber daya komputasi yang umum ditemukan dalam environment training modern. Mixed precision training dengan FP16 diimplementasikan untuk mengurangi memory usage hingga 50 persen tanpa mengorbankan kualitas hasil pelatihan, memungkinkan training model yang lebih besar dengan resource yang terbatas. Gradient accumulation dengan 8 steps digunakan untuk mencapai effective batch size yang lebih besar tanpa memory overhead, di mana setiap device menggunakan batch size 1 tetapi mengakumulasi gradients untuk 8 steps sebelum melakukan parameter update. Gradient checkpointing diaktifkan untuk melakukan trade-off antara memory dan computation time, memungkinkan training pada model yang lebih besar dengan memory yang terbatas dengan menghitung ulang aktivasi tertentu selama backward pass. Memory management juga dioptimalkan dengan menonaktifkan pin memory dan multiprocessing pada dataloader untuk mengurangi overhead memory, serta melakukan explicit CUDA cache cleaning dan garbage collection secara berkala untuk menjaga stabilitas penggunaan memory.

Sistem monitoring hybrid mengintegrasikan real-time evaluation dengan session management yang comprehensive untuk memastikan visibility yang optimal terhadap proses pelatihan. Monitoring mencakup tracking GPU memory utilization secara real-time untuk memastikan penggunaan resource yang optimal dan mencegah out-of-memory errors yang dapat menghentikan proses pelatihan. Sample prediction evaluation dilakukan secara berkala setiap 5 epoch untuk monitoring kualitas model selama proses pelatihan, dengan evaluasi pada sample validation yang representatif untuk memberikan insight tentang progress pelatihan dan deteksi dini terhadap masalah seperti overfitting. Sistem juga menyediakan automatic checkpoint management dengan download link otomatis untuk setiap checkpoint yang disimpan, memudahkan akses dan backup model pada berbagai tahap pelatihan. Session keep-alive mechanism diimplementasikan menggunakan background thread untuk mencegah session timeout pada training jangka panjang, terutama penting untuk environment seperti Kaggle yang memiliki batasan waktu session, dengan heartbeat mechanism yang secara berkala memberikan signal bahwa session masih aktif.

Fungsi collation menggunakan strategi yang terbukti efektif dalam menangani batch processing dan format input yang konsisten untuk memastikan kualitas data yang masuk ke model. Strategi ini memastikan setiap generation dimulai dengan task identifier yang tepat melalui consistent task prompting, di mana token pertama dari decoder input selalu di-force menjadi task start token untuk memberikan konteks yang jelas tentang tugas yang harus dilakukan model. Proper label masking diterapkan dengan menggunakan nilai -100 untuk padding tokens agar tidak berkontribusi dalam loss calculation, memastikan model hanya belajar dari token yang valid dan tidak terdistorsi oleh padding yang bersifat artificial. Batch consistency handling diimplementasikan untuk menangani samples dengan ukuran berbeda dalam single batch, dengan padding dan masking yang appropriate untuk menjaga konsistensi processing dan memastikan semua samples dalam batch dapat diproses dengan benar tanpa error dimensi.

Sistem evaluasi hybrid menggunakan field-specific accuracy measurement yang memberikan insight mendalam tentang performa model pada setiap aspek ekstraksi informasi yang menjadi target. Field-level accuracy dihitung untuk setiap field target secara individual, memungkinkan identifikasi field mana yang paling sulit diekstraksi dan memerlukan improvement lebih lanjut, seperti apakah model lebih kesulitan dalam mengekstrak informasi numerik seperti total\_amount atau informasi tekstual seperti target\_name. Overall accuracy menggunakan exact match accuracy untuk keseluruhan structured output, memberikan gambaran performa model secara holistik dalam menghasilkan output yang sepenuhnya benar. Robust decoding dengan fallback mechanism diimplementasikan untuk handling invalid tokens yang mungkin dihasilkan selama inference, memastikan sistem tetap dapat berfungsi meskipun ada output yang tidak sepenuhnya valid dengan menggunakan clipping dan nan-to-num conversion untuk menangani token yang berada di luar vocabulary range.

Implementasi hybrid ini berhasil mengombinasikan robustness dari multiple approaches sambil mempertahankan efficiency dan effectiveness dalam proses fine-tuning model Donut untuk domain bukti pembayaran. Pendekatan ini memungkinkan pelatihan yang stabil dengan resource yang terbatas, monitoring yang comprehensive untuk debugging dan optimization, serta hasil yang konsisten dan dapat diandalkan untuk deployment dalam aplikasi production. Keberhasilan implementasi ini ditunjukkan melalui kemampuan sistem untuk menangani berbagai skenario error, optimisasi penggunaan memory yang efektif, dan hasil pelatihan yang converge dengan baik dalam jumlah epoch yang reasonable, memberikan foundation yang solid untuk tahap selanjutnya dalam pipeline pengembangan sistem.
