\subsection{Perancangan Konversi dan Kompresi Model}
\label{subsec:perancangan-konversi-dan-kompresi-model}

 Proses konversi dan kompresi model \donut{} dari format PyTorch orisinil ke format yang \emph{optimized} untuk \emph{on-device inference}. Proses tersebut diperlukan untuk mencapai tujuan model yang dapat digunakan pada perangkat \emph{mobile} Android. Subbab ini menjelaskan pendekatan yang digunakan untuk konversi model ke format \emph{Open Neural Network Exchange} (ONNX) dan strategi kompresi yang akan diterapkan pada model.

\subsubsection{Perancangan Konversi dan Kompresi Model \donut{} ke ONNX}
\label{subsubsec:strategi-konversi-onnx}

\emph{Open Neural Network Exchange} (ONNX) dipilih sebagai target format konversi utama karena memberikan keunggulan dalam hal portabilitas lintas \emph{platform} dan optimasi \emph{inference}. ONNX menyediakan dukungan luas untuk berbagai \emph{framework} dan perangkat keras, sehingga memungkinkan integrasi yang lebih baik dengan \emph{platform mobile}. Perubahan format menuju \onnx{} merupakan langkah penting untuk memastikan model dapat dioptimalkan.

Arsitektur model \donut{} yang berbasis \textit{VisionEncoderDecoderModel} bukan merupakan arsitektur yang umum untuk dikonversi, sehingga konversi ke ONNX memerlukan pendekatan yang lebih kompleks dibandingkan model lainnya. \onnx{} telah menyediakan \emph{export tool}{}\footnote{\url{https://huggingface.co/spaces/onnx/export}} yang dapat digunakan untuk mengonversi model PyTorch ke format ONNX. \emph{Export tool} ini akan digunakan untuk melakukan konversi model \donut{} ke format ONNX dan model tersebut yang akan digunakan sebagai dasar untuk kompresi model lebih lanjut. 

\subsubsection{Strategi Kompresi dan Kuantisasi Model}
\label{subsubsec:strategi-kompresi-dan-kuantisasi-model}

Model \donut{} yang telah dikonversi ke format \onnx{} akan melalui tahap kompresi untuk mengurangi ukuran model dan meningkatkan kecepatan inferensi pada perangkat mobile. Kompresi ini penting untuk memastikan model dapat berjalan efisien pada perangkat dengan keterbatasan sumber daya, seperti pada perangkat Android. Metode yang digunakan untuk kompresi dan kuantisasi adalah \emph{Dynamic Quantization}. \emph{Dynamic quantization} dipilih sebagai pendekatan utama untuk melakukan kompresi dan kuantisasi model \donut. Metode \emph{Dynamic Quantization} yang dapat diterapkan adalah FP16, INT8, dan UINT8. Pendekatan ini memungkinkan model untuk tetap mempertahankan akurasi yang tinggi sambil mengurangi ukuran model secara signifikan.
