\subsection{Perancangan \emph{Fine-Tuning} Model \donut}
\label{subsec:perancangan-fine-tuning-model}

Subbab ini menjelaskan rancangan proses \emph{fine-tuning} model \donut{} untuk tugas ekstraksi data bukti pembayaran. Perancangan ini mencakup pemilihan model dasar, konfigurasi arsitektur, desain format keluaran, strategi \emph{preprocessing} data, dan konfigurasi pelatihan untuk melakukan \emph{fine-tuning} model \donut{} pada \dataset{} yang telah dipersiapkan. Proses ini bertujuan untuk menghasilkan model yang mampu melakukan ekstraksi informasi dari bukti pembayaran dengan akurasi tinggi, serta dapat diintegrasikan ke dalam aplikasi \emph{mobile}.

\subsubsection{Pemilihan Model Dasar dan Konfigurasi Awal}
\label{subsubsec:pemilihan-model-dasar}

Model dasar yang dipilih untuk proses \emph{fine-tuning} pada \dataset{} bukti pembayaran QRIS dan transfer adalah model \donut{} yang telah di-\emph{fine-tune} pada \dataset{} \linebreak CORD (\emph{Consolidated Receipt Dataset}) - v2 yang telah disediakan oleh naver-cloud-ix. Model tersebut diberi alias \donutcord. Model \donutcord{} dipilih dengan mempertimbangkan faktor-faktor berikut:

\begin{enumerate}
    \item \textbf{\emph{Domain similarity}}: Model \donutcord{} telah dilatih pada \dataset{} dokumen terstruktur yang memiliki kemiripan dengan struk dan bukti pembayaran dalam hal tata letak dan struktur informasi.
    \item \textbf{\emph{Pre-trained capabilities}}: Model ini telah memiliki kemampuan dasar dalam memahami dokumen semi-terstruktur, sehingga dapat mempercepat proses konvergensi selama \emph{fine-tuning}.
    \item \textbf{Arsitektur optimal}: Kombinasi \swin{} sebagai \emph{encoder} visual dan \bart{} sebagai \emph{decoder} teks telah terbukti efektif untuk tugas \emph{Visual Document Understanding}.
\end{enumerate}

Konfigurasi awal model melibatkan ekspansi \emph{vocabulary tokenizer} untuk menambahkan token khusus domain pembayaran. Token yang ditambahkan berjumlah 14 token khusus yang terdiri dari:
\begin{enumerate}
    \item Token pembuka dan penutup untuk setiap \emph{field}
    \begin{enumerate}
    \item \texttt{<s\_total\_amount>}, \texttt{</s\_total\_amount>}
    \item \texttt{<s\_transaction\_time>}, \texttt{</s\_transaction\_time>}
    \item \texttt{<s\_transaction\_identifier>}, \texttt{</s\_transaction\_identifier>}
    \item \texttt{<s\_type>}, \texttt{</s\_type>}
    \item \texttt{<s\_target\_name>}, \texttt{</s\_target\_name>}
    \item \texttt{<s\_application>}, \texttt{</s\_application>}
    \end{enumerate}
    \item Token \emph{task identifier}~\\ \texttt{<s\_payment\_proof>}, \texttt{</s\_payment\_proof>}
\end{enumerate}

Ekspansi \emph{vocabulary} ini memerlukan penyesuaian dimensi \emph{embedding layer} pada \decoder{} dan konfigurasi ulang parameter \texttt{decoder\_start\_token\_id} untuk memastikan model memulai generasi dengan token \emph{task} yang tepat.

Format keluaran dari model juga perlu direstrukturisasi untuk memungkinkan ekstraksi informasi yang konsisten dan otomatis. Format yang dirancang menggunakan \emph{markup} berbasis XML dengan struktur hierarkis sebagai berikut:

\begin{verbatim}
<s_payment_proof>
<s_total_amount>nilai_total</s_total_amount>
<s_transaction_time>waktu_transaksi</s_transaction_time>
<s_transaction_identifier>id_transaksi</s_transaction_identifier>
<s_type>jenis_transaksi</s_type>
<s_target_name>nama_tujuan</s_target_name>
<s_application>aplikasi_pembayaran</s_application>
</s_payment_proof>
\end{verbatim}

Token \texttt{<s\_payment\_proof>} berfungsi sebagai task prompt yang memberikan konteks kepada model bahwa tugas yang dijalankan adalah ekstraksi informasi dari dokumen bukti pembayaran yang dapat berupa QRIS dan transfer dan berbeda dari tugas ekstraksi dokumen struk pembayaran pada \donutcord. Dengan demikian, model diarahkan untuk menghasilkan keluaran yang sesuai dengan format yang telah ditentukan.

\subsubsection{Strategi Preprocessing Data}
\label{subsubsec:strategi-preprocessing-data}

Mengingat karakteristik data metadata yang berpotensi mengalami korupsi atau format yang tidak konsisten, penulis merancang strategi preprocessing yang robust dengan beberapa tahapan:

\paragraph{Pembersihan Metadata}
Penulis mengimplementasikan parser multi-tahap untuk menangani file metadata yang berformat JSONL tetapi mengalami masalah formatting:
\begin{enumerate}
    \item \textbf{Parser Array JSON}: Mencoba parsing sebagai array JSON lengkap setelah menambahkan delimiter koma.
    \item \textbf{Parser Line-by-Line}: Jika gagal, melakukan parsing objek JSON per baris dengan tracking brace count untuk rekonstruksi objek yang terpisah.
    \item \textbf{Error Recovery}: Menangani karakter khusus dan trailing commas yang dapat menyebabkan parsing error.
\end{enumerate}

\paragraph{Pencocokan File Gambar}
Sistem pencarian file gambar yang fleksibel diimplementasikan untuk mengatasi inkonsistensi penamaan file:
\begin{itemize}
    \item Pencocokan direct path
    \item Pencocokan case-insensitive untuk handling variasi kapitalisasi
    \item Pencocokan dengan berbagai ekstensi gambar (.jpg, .jpeg, .png)
    \item Pencocokan partial untuk nama file tanpa ekstensi
\end{itemize}

\paragraph{Validasi dan Pembersihan Data}
Setiap sample data divalidasi untuk memastikan:
\begin{itemize}
    \item Keberadaan file gambar yang valid
    \item Kelengkapan ground truth annotation
    \item Konsistensi format field value (normalisasi spasi internal)
    \item Handling nilai null atau kosong
\end{itemize}

\subsubsection{Konfigurasi Pelatihan dan Optimisasi}
\label{subsubsec:konfigurasi-pelatihan}

Penulis merancang konfigurasi pelatihan yang mempertimbangkan keterbatasan sumber daya komputasi sambil memaksimalkan kualitas hasil \emph{fine-tuning}:

\paragraph{Hyperparameter Selection}
\begin{itemize}
    \item \textbf{Learning Rate}: 3e-5, dipilih sebagai trade-off antara kecepatan konvergensi dan stabilitas pelatihan
    \item \textbf{Batch Size}: 1 per device dengan gradient accumulation 8 steps, memberikan effective batch size 8
    \item \textbf{Epochs}: 40 dengan early stopping patience 3 untuk mencegah overfitting
    \item \textbf{Weight Decay}: 0.01 untuk regularisasi model
\end{itemize}

\paragraph{Optimisasi Memori}
Untuk mengatasi keterbatasan memori GPU, penulis mengimplementasikan beberapa teknik optimisasi:
\begin{enumerate}
    \item \textbf{Mixed Precision Training}: Menggunakan FP16 untuk mengurangi penggunaan memori hingga 50\%
    \item \textbf{Gradient Checkpointing}: Menukar komputasi tambahan dengan penghematan memori
    \item \textbf{Memory Management}: Disable pin memory dan multiprocessing pada dataloader
    \item \textbf{Garbage Collection}: Pembersihan cache CUDA secara eksplisit
\end{enumerate}

\paragraph{Data Collation Strategy}
Penulis merancang custom collation function yang menangani:
\begin{itemize}
    \item Forcing decoder input dengan task start token
    \item Proper label masking untuk padding tokens (-100 untuk ignore dalam loss calculation)
    \item Batch consistency handling untuk samples dengan ukuran berbeda
\end{itemize}