\subsection{Perancangan Fine-tuning}
\label{subsec:perancangan-fine-tuning}

Perancangan strategi \emph{fine-tuning} merupakan tahapan kritis yang menentukan kualitas adaptasi model terhadap domain pembayaran Indonesia. Strategi ini dirancang untuk mengoptimalkan kinerja model sambil mencegah \emph{overfitting} dan memastikan efisiensi komputasi.

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.8\textwidth]{images/training-process-flow.png}
%     \caption{Alur kerja proses fine-tuning model}
%     \label{fig:training-process-flow}
% \end{figure}

\autoref{fig:training-process-flow} menggambarkan proses \emph{fine-tuning} yang sistematis, dimulai dari inisialisasi model dasar hingga penyimpanan model yang telah dilatih. Proses ini dirancang dengan mempertimbangkan karakteristik \dataset{} dan sumber daya komputasi yang tersedia.

Konfigurasi pelatihan ditetapkan berdasarkan praktik terbaik dalam \emph{fine-tuning} model \transformer{} untuk tugas pemahaman dokumen. Jumlah \emph{epoch} ditetapkan sebanyak 30 berdasarkan beberapa pertimbangan utama. Pertama, penelitian pada model \donut{} menunjukkan bahwa konvergensi optimal untuk tugas \emph{fine-tuning} umumnya tercapai dalam rentang 20-40 \emph{epoch} \parencite{kim2021donut}. Kedua, dengan ukuran \dataset{} sekitar 300 sampel, 30 \emph{epoch} memberikan sekitar 10.000 iterasi parameter update yang sufficient untuk adaptasi domain tanpa risiko \emph{overfitting} yang signifikan. Ketiga, implementasi \emph{early stopping} dengan \emph{patience}=3 memungkinkan penghentian pelatihan secara otomatis jika tidak ada peningkatan pada \emph{validation loss} selama 3 \emph{epoch} berturut-turut, mencegah pemborosan sumber daya komputasi.

Strategi optimasi menggunakan \emph{batch size} 1 dengan \emph{gradient accumulation} sebanyak 8 langkah untuk mensimulasikan efek \emph{batch size} yang lebih besar sambil mengakomodasi keterbatasan memori GPU. \emph{Learning rate} ditetapkan pada 3e-5 sebagai titik optimal antara kecepatan konvergensi dan stabilitas pelatihan. Penggunaan \emph{mixed precision} FP16 memungkinkan pelatihan yang lebih efisien secara memori tanpa mengorbankan akurasi model.

Monitoring pelatihan dilakukan melalui pelacakan metrik validasi secara berkala, termasuk \emph{loss}, akurasi, dan metrik evaluasi khusus seperti \mcer. Implementasi \emph{checkpoint} otomatis setiap 5 \emph{epoch} memastikan bahwa progres pelatihan tidak hilang jika terjadi gangguan sistem. Strategi ini juga memungkinkan analisis evolusi kinerja model sepanjang proses pelatihan dan pemilihan \emph{checkpoint} optimal berdasarkan kinerja validasi.

Validasi robustness model dilakukan melalui evaluasi pada \dataset{} validasi yang mencakup berbagai variasi dokumen pembayaran. Proses ini memastikan bahwa model tidak hanya menghasilkan akurasi tinggi pada data pelatihan, tetapi juga mampu generalisasi dengan baik pada data yang belum pernah dilihat sebelumnya.
