%-------------------------------------------------------------------%
% Bibliography file for Vincent Franstyo's Thesis
% Topic: OCR-free Payment Receipt Data Extraction System
%-------------------------------------------------------------------%

% Core Donut Paper
@article{kim2021donut,
  title   = {Donut: Document understanding transformer without ocr},
  author  = {Kim, Geewook and Hong, Teakgyu and Yim, Moonbin and Park, Jinyoung and Yim, Jinyeong and Hwang, Wonseok and Yun, Sangdoo and Han, Dongyoon and Park, Seunghyun},
  journal = {arXiv preprint arXiv:2111.15664},
  volume  = {7},
  number  = {15},
  pages   = {2},
  year    = {2021}
}

@article{han2021transformer,
  title   = {Transformer in transformer},
  author  = {Han, Kai and Xiao, An and Wu, Enhua and Guo, Jianyuan and Xu, Chunjing and Wang, Yunhe},
  journal = {Advances in neural information processing systems},
  volume  = {34},
  pages   = {15908--15919},
  year    = {2021}
}


% Transformer Architecture
@article{vaswani2017attention,
  title   = {Attention is all you need},
  author  = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal = {Advances in neural information processing systems},
  volume  = {30},
  year    = {2017}
}


% Swin Transformer
@article{liu2021swin,
  title   = {Swin transformer: Hierarchical vision transformer using shifted windows},
  author  = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  journal = {Proceedings of the IEEE/CVF international conference on computer vision},
  year    = {2021}
}

% BART
@article{lewis2019bart,
  title   = {BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  author  = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Veselin and Zettlemoyer, Luke},
  journal = {arXiv preprint arXiv:1910.13461},
  year    = {2019}
}

% Deep Learning Book
@book{Goodfellow-et-al-2016,
  title     = {Deep Learning},
  author    = {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
  publisher = {MIT Press},
  note      = {\url{http://www.deeplearningbook.org}},
  year      = {2016}
}

% CRISP-DM Methodology
@inproceedings{saltz2021crisp,
  title        = {CRISP-DM for Data Science: Strengths, Weaknesses and Potential Next Steps},
  author       = {Saltz, Jeffrey S},
  booktitle    = {2021 IEEE International Conference on Big Data (Big Data)},
  pages        = {2337--2344},
  year         = {2021},
  organization = {IEEE}
}

% LayoutLM (for comparison)
@article{xu2020layoutlm,
  title   = {LayoutLM: Pre-training of text and layout for document image understanding},
  author  = {Xu, Yiheng and Li, Minghao and Cui, Lei and Huang, Shaohan and Wei, Furu and Zhou, Ming},
  journal = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  year    = {2020}
}

% Document AI Survey
@article{katti2018chargrid,
  title   = {Chargrid: Towards understanding 2d documents},
  author  = {Katti, Anoop R and Reisswig, Christian and Guder, Cordula and Brarda, Sebastian and Bickel, Steffen and H{\"o}hne, Johannes and Faddoul, Jean Baptiste},
  journal = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  year    = {2018}
}

% Computer Vision for Document Analysis
@article{alzubaidi2021review,
  title     = {Review of deep learning: concepts, {CNN} architectures,
               challenges, applications, future directions},
  author    = {Alzubaidi, Laith and Zhang, Jinglan and Humaidi, Amjad J and
               Al-Dujaili, Ayad and Duan, Ye and Al-Shamma, Omran and
               Santamar{\'\i}a, J and Fadhel, Mohammed A and Al-Amidie, Muthana
               and Farhan, Laith},
  abstract  = {In the last few years, the deep learning (DL) computing paradigm
               has been deemed the Gold Standard in the machine learning (ML)
               community. Moreover, it has gradually become the most widely
               used computational approach in the field of ML, thus achieving
               outstanding results on several complex cognitive tasks, matching
               or even beating those provided by human performance. One of the
               benefits of DL is the ability to learn massive amounts of data.
               The DL field has grown fast in the last few years and it has
               been extensively used to successfully address a wide range of
               traditional applications. More importantly, DL has outperformed
               well-known ML techniques in many domains, e.g., cybersecurity,
               natural language processing, bioinformatics, robotics and
               control, and medical information processing, among many others.
               Despite it has been contributed several works reviewing the
               State-of-the-Art on DL, all of them only tackled one aspect of
               the DL, which leads to an overall lack of knowledge about it.
               Therefore, in this contribution, we propose using a more
               holistic approach in order to provide a more suitable starting
               point from which to develop a full understanding of DL.
               Specifically, this review attempts to provide a more
               comprehensive survey of the most important aspects of DL and
               including those enhancements recently added to the field. In
               particular, this paper outlines the importance of DL, presents
               the types of DL techniques and networks. It then presents
               convolutional neural networks (CNNs) which the most utilized DL
               network type and describes the development of CNNs architectures
               together with their main features, e.g., starting with the
               AlexNet network and closing with the High-Resolution network
               (HR.Net). Finally, we further present the challenges and
               suggested solutions to help researchers understand the existing
               research gaps. It is followed by a list of the major DL
               applications. Computational tools including FPGA, GPU, and CPU
               are summarized along with a description of their influence on
               DL. The paper ends with the evolution matrix, benchmark
               datasets, and summary and conclusion.},
  journal   = {J. Big Data},
  publisher = {Springer Science and Business Media LLC},
  volume    = 8,
  number    = 1,
  pages     = {53},
  month     = mar,
  year      = 2021,
  keywords  = {Convolution neural network (CNN); Deep learning; Deep learning
               applications; Deep neural network architectures; FPGA; GPU;
               Image classification; Machine learning; Medical image analysis;
               Supervised learning; Transfer learning},
  copyright = {https://creativecommons.org/licenses/by/4.0},
  language  = {en}
}

@inproceedings{xie2021oriented,
  title     = {Oriented R-CNN for object detection},
  author    = {Xie, Xingxing and Cheng, Gong and Wang, Jiabao and Yao, Xiwen and Han, Junwei},
  booktitle = {Proceedings of the IEEE/CVF international conference on computer vision},
  pages     = {3520--3529},
  year      = {2021}
}


% CNN Architecture
@article{lecun1998gradient,
  title     = {Gradient-based learning applied to document recognition},
  author    = {LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal   = {Proceedings of the IEEE},
  volume    = {86},
  number    = {11},
  pages     = {2278--2324},
  year      = {1998},
  publisher = {IEEE}
}

% Mobile ML/ONNX
@article{jouppi2017datacenter,
  title     = {In-datacenter performance analysis of a tensor processing unit},
  author    = {Jouppi, Norman P and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and others},
  journal   = {ACM SIGARCH Computer Architecture News},
  volume    = {45},
  number    = {2},
  pages     = {1--12},
  year      = {2017},
  publisher = {ACM}
}

@article{diwan2023object,
  title     = {Object detection using YOLO: Challenges, architectural successors, datasets and applications},
  author    = {Diwan, Tausif and Anirudh, G and Tembhurne, Jitendra V},
  journal   = {multimedia Tools and Applications},
  volume    = {82},
  number    = {6},
  pages     = {9243--9275},
  year      = {2023},
  publisher = {Springer}
}


@article{peffers2007dsrm,
  author  = {Peffers, Ken and Tuunanen, Tuure and Rothenberger, Marcus and Chatterjee, S.},
  year    = {2007},
  month   = {01},
  pages   = {45-77},
  title   = {A design science research methodology for information systems research},
  volume  = {24},
  journal = {Journal of Management Information Systems}
}

@inproceedings{wang2019convolutional,
  title        = {Convolutional recurrent neural networks for text classification},
  author       = {Wang, Ruishuang and Li, Zhao and Cao, Jian and Chen, Tong and Wang, Lei},
  booktitle    = {2019 international joint conference on neural networks (IJCNN)},
  pages        = {1--6},
  year         = {2019},
  organization = {IEEE}
}
